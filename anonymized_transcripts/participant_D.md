# Appendix D: Interview Transcript of Participant D

**Interview Location:** Zoom  
**Mode:** Online  
**Date:** 15/05/2025  
**Participant Role:** Lead Engineer at the case fintech

---


**Interviewer**: Okay, cool. Very interesting. So I'll just jump right at it. So since you are very into architecture, I had a very high-level architecture in my project brief. Did you look at it and what do you think of the feasibility of the project, you know, based on that flow?

**Participant D**: Just give me a sec, let me pull out the architecture.

**Interviewer**: Yeah, let me share. So this is the idea of, and when I say architecture, it's like the entire flow. The goal of the project right now is we think observability is evolving to a point where it's not just about collecting logs and telemetry, right? People want to—people think that there should be business metrics, and it should be easy to correlate observability data to their business implications, and that's like the background to this project. And yes, so, um, in my head, I think like an agentic RAG architectural work, and where we have like different agents managed by LangChain, and then we can communicate by APIs to this different. So Datadog, a redshift. And maybe we have, like, a vector database with product documentation, incident reports, and all of that. And we can correlate everything to, like, get insights from data. 

So we're using natural language queries to, like, actually analyze data from, like, these sources cohesively. So I would say, like, based on this, I mean, it's not detailed, but, like, one, what do you think is—do you think it's a feasible project? Maybe, like, from a technical perspective, and do you think there's something off or something that should change about how we're thinking about it now or how I'm thinking about it?

**Participant D**: No, I think this is feasible. One thing that I can comment is, you know, just nowadays, it has just become much easier because of MCPs. So you can just have MCP servers, and then you don really need that other agent as such to decide because you can just attach different data sources as MCP servers and then provide that to your actual agent. You still have to give your agent some context about the kind of queries that each of the tools can handle. For example, if you ask about what the company policies are, the agent should have sufficient context to know that, oh, you know what, policy, maybe this falls more under knowledge base rather than business metrics, things like that. So, yes, you will need a router. So not necessarily… Yeah, so the role will not be of a router per se, but it can be more of a planning agent because to answer certain queries, it might require that you have to use multiple tools.

For example, if I ask you, "Explain what PIV is and what was our PIV for the last three months." So in that case, you have to go to the knowledge base, get the answer for what a PIV is. You have to go to the business metrics agent, fetch PIV for the last three months, and then provide that output, right? The final output. So, yeah, so it would be more of a planning agent, and what you are giving that agent is a set of tools. You don't need specialized agents for those use cases now.

**Interviewer**: Okay. You are the second person that has mentioned MCP servers to me, so I'll definitely be checking them out.

**Participant D**: Yeah.

**Interviewer**: Okay. So what I hear is that this is technically feasible. Yes, but it can be simplified because of MCPs.

**Interviewer**: Okay. Okay. So as the lead of platforms, if I came to you today and I said, okay, that I wanted to deploy this, you know, like based on the company's AI readiness today or based on what we currently have, how would you recommend the infrastructure? So for instance, I've talked to somebody who says, you know, based on what we have today, like just go to Bedrock; we probably use Bedrock for everything—knowledge base, vector database, and all of that. Do you share the same opinion?

**Participant D**: Not really. If you have to deploy this, yes, you just need an application. One thing that I would agree with is that you don't have to deploy LLMs locally or on some EC2 server. It is much cheaper and easier to use models that are already hosted and leverage the APIs. So yes, Bedrock APIs or OpenAI APIs, directly leverage those APIs instead of deploying your own model. And then the other thing that I would suggest also is,

**Participant D**: Yeah, for knowledge base and vector database, I am, for the embedding models, still use the hosted versions, like, you know, OpenAI, Bedrock, they both provide embedding models as well. So we use the hosted APIs for them. They are quite cheap as well. I'm not sure if Bedrock has knowledge base or vector DB features. So I'm not sure about it. But you can deploy a Postgres database on RDS, which comes with PGVector, which is a vector database extension that you can directly use. You can use MongoDB as a vector data store as well now. So there are enough database technologies that can act as vector stores right now. So you can use any of those.

**Interviewer**: Okay, but I mean, I do think that Bedrock has like knowledge bases. Like, I think they have a lot.

**Participant D**: But how do you keep the data in sync?

**Interviewer**: I think you can—I think they do—you can use S3 for like multimodal data, and then you can actually use, okay, so they have two APIs: retrieve API and retrieve and generate. I think the retrieve API uses just S3, and then you can actually decide to have your own offering of vector database, some of the popular ones to choose from. So I know that they have like—I've not played with it, but I know that they have like a full RAG pipeline.

**Participant D**: Yeah, I see they have a RAG pipeline, and they have a knowledge base feature. Yeah, I think it's good if they have that support. For me, some of the nuances of a RAG system, I'm not sure how they handle that information. Like, you know, for example, if you go and ask them, "How was the offsite?" Now, if you look at the knowledge base, it would have data for the first offsite, the latest offsites, and then all the other offsites in the middle, right? So now if I ask it, "What is the schedule for this offsite? When is the offsite happening?" You know, it needs to have enough context to say, "Oh, yeah, I have all of this information about offsites, but this is the latest offsite. You know, this is the information about the latest offsite that the user is interested in." So sort of what a lot of RAG systems miss nowadays is information about time and, like, you know, tagging what sort of information is outdated and what is the more recent version. That is what I've seen these systems struggle with nowadays. So yeah, if they can handle things like that, summarize, answer questions.

**Participant D**: Yeah, I think, yeah, you can—you can use Bedrock for this.

**Interviewer**: Yeah, okay, but I think actually it's honestly a question of, um, you know, just feasibility and capacity. If I said that I wanted to do this now in observability, then my observability—my natural our observability duties will suffer. So I think like it's just considering the company's maturity, right? And, and since you are like, um, on the platform, so maybe I should ask you like where do you think is there a—is a clear roadmap for how we intend to use like AI the company because, um, you know, people recommend using a platform approach where you have like centralized data, central governance, and you know people can just find easy ways to plug in their use cases. i don't know that we  do that currently   


**Participant D**: So yes, so we do have some of that. So it is not rolled out to everyone, but we do have—so what we do have is a central place where we—so what I have deployed, it is in the sandbox environment right now, is an API where we can give people API keys; we can give people access. And behind the scenes, we can configure all the models as well. So I have an AI proxy which connects to OpenAI and Bedrock behind the scenes for models. And then you can just use an API key and use it. You can create teams. You can assign budgets to the teams. You can track usage. You can track which API keys are being used. And people then have a flexibility of models instead of just sticking to one provider. 

Also, it makes it very easy for us as well to provide access to new models because models change. There are different companies like DeepSeek, Gemini, that are sort of pushing the boundaries with faster models, cheaper models, which would be good to use. We don't want us to be limited to just a single model provider or single LLM provider. So having something like a proxy helps us to sort of enable engineers to build applications or even like for people to use tools that require AI API keys to just leverage that. And also from a company perspective, have like SSO over there and manage budgets, manage allocation, things like that at a central place. So, yeah. So that is something that we do have.

 And then on top of it, what I have done is I have deployed a ChatGPT-like UI as well, open source, that we can just use. So again, as part of that, people don't have to go to ChatGPT and access ChatGPT. We can have our own internal tool that people can use. People can add prompts, share prompts, things like that. It is sort of like an internal alpha version right now. But yeah, we do have plans to sort of open it up to more users.

**Interviewer**: Okay, so do we have like, um, production use cases on that environment now?

**Participant D**: No, we are not using it for anything production right now, using it for mainly some internal use cases, for like coding agents and things like that is what I was using it for for the most part, but not for, so mainly for internal efficiency, internal operations, not for production yet, because as I said, it is on the sandbox environment.

**Interviewer**: Yeah, so it still looks like an exploration. Okay. So what would you say about the data side? Is there like, does Platform have, because I mean, judging from what I've heard, it doesn't seem like, one, the company doesn't have, seems to have like a big data strategy. It seems like we're still very—data team is on, you know, business data, observability team is on, and observability data, but like, since we're thinking AI, is there a, is there any plan to just harness big data, like just bring everything together, and, um…

**Participant D**: So, so the term that you are looking for is called feature stores. Okay, where like, so the feature store, you have a feature store, where people can access data and then build models on top of. No, I know we need to get there, but, but we are not there yet, mainly because of resource constraints at the moment.

**Interviewer**: Okay, okay, cool. Um, that is good. So basically, if you look at the project, um, mainly from a technical perspective, would you have any concerns? First of all, do you think that it is something that is beneficial for us, maybe at this stage at the company?

**Participant D**: Yes, definitely. So knowledge base, if you can, if you can expose that information, I think it definitely helps time to information for people that are trying to understand or get answers to questions. Like, you know, I have new joiners that almost ask me same or similar questions all the time. You know, we can help answer some of those questions for them. Similarly, when there are incidents, or when we need more information about something, like, you know, if you can make data accessible to people, like, you know, it empowers them to sort of make those decisions by themselves rather than just relying on someone else. Like, you know, going on Team Observability and then asking, "Hey, what's happening?" But, you know, if staging is down, going and asking DevOps, "Hey, why is staging down? You know, can you give me the logs?" And, you know, like a person has to go and actually do those tasks for you. But instead, you know, if I can, if I can just ask on Slack, and AI can answer, that definitely reduces the dependency for me.

**Interviewer**: Okay, makes sense. So would you have any specific concerns or risks associated with the projects like from a technical perspective?

**Participant D**: No, not really. No concerns from there.

**Interviewer**: Okay. And maybe generally AI, right? I mean, people still have like their reservations about just how reliable LLMs are.

**Participant D**: Yes, which is fine, which is okay, but you need to also understand that you can't expect it to do everything or anything, right? Like, you know, it is more important, you need to also see that, you know, it helps improve and increase productivity as well. Like, yeah, the accuracy may be bad, but the speed of response is fast enough for you to iterate on it, right? Like, you know, if I, if I want to understand what is happening with Beanstalk logs, and a human takes 20 minutes to send me the first response, and then that human also is spending 10 minutes, and then on the follow-up, getting onto the thread, if that thing is taking me, let's say, overall 45 minutes. If I can ask an AI that which gives me a response in under one minute, and I can have 10 back and forth with it, I am getting 70% or 80% there within five minutes, something that takes me around 45 minutes to do with a human in the loop. 

So it does unlock possibilities. And I think what is more important is for people to continue to use the technology, get better with it, understand how you can extract the most value out of it rather than saying it is not accurate and I can't use it because models are going to get better over time, technology is going to improve over time. I still feel we are sort of in the dial-up stage of LLMs right now, you know, where it is still quite expensive, not as widely used by everyone. But, you know, once you get to the broadband stage of LLMs, you don't want to be in a position where you don't understand how the technology works or how to interact with it, where other people are able to extract more value out of the technology compared to you, because that will be the differentiating factor. 

I'm already seeing coding interviews where it's not about how can you code without AI, it's about how can you use AI to do your job faster, right? And that is how, that is where the world is moving and I think that is where it will move eventually. So even for my own purposes, you know, I used to do research and like, you know, open 10 different tabs and then, you know, try to figure out, okay, how everything works and what happens to it. Now for me, like, you know, when I'm in my commute or when I am driving or walking, if I want to research something, I just go to Perplexity, type or something on deep research, and within 15 minutes something is ready. I read it, I am 60, there, 60, 70% of my research is done. And then what I am doing from there is just asking follow-up questions and trying to answer follow-up questions for myself, right? So it's about how can you use it to improve your productivity.

**Interviewer**: Okay, and do you have recommendations, just like you said, like, um, you know, people need to catch on and it's almost not an option anymore, but like, would you have like recommendations on how, you know, employees can just, or I mean, maybe not necessarily employees, like the entire company, you know, to ensure that we are moving as fast as we should when it comes to adoption, because it currently looks like there is no—I mean, I've seen like some exploration going on, especially with AI and everything, but like, um, do you think like that's enough for the pace that we need to—

**Participant D**: There is, there is no recipe per se. I think, I think the most important thing is for people to—for the company to create space for exploration for the people to continue their exploration and then share what their experiences have been with each other. That is the only way you can sort of extract the most value out of it, because it is something that is very dynamic at the moment. You know, you can't say there are certain best practices that are very clear or certain best practices that everyone should do or not do. Like, you know, the space is evolving really fast. There are no, there is no clear Google company like Google or clear company like OpenAI that is winning everything or that is taking most of the market share where we can say, you know what, let's bet on these companies. Things are changing very quickly around these things. So people just have to continue to play with things and then share. And that is a space that we have to create.

**Interviewer**: And so you think, and maybe just bringing it down to this use case now, do you think there are specific factors that would make these projects quickly go from maybe an experiment? Because I think in general, generally the perception is that many GenAI projects just get stuck in experimentation and never make it to production. Um, and I can ask you generally, but I would say like specifically for this use case, um, do you think that there are things that can mitigate like, I don't, I don't just start working on this and it never crosses if—

**Participant D**: If you build something that is delivering value to the people, then people will ask for features, people will report, and, you know, it's very easy to sort of communicate the impact that that tool has on everyone, right? Instead of, instead of saying that, okay, yeah, like the thing is if you, if you think of this as a project deliverable and say like, okay, I've done the implementation nowadays onto the users. I think in order for any project to survive, any project or any internal tool to survive, two things are important. You have to build something that actually solves a problem and you have to continue to provide support to the users. You can't say that your job is done. For example,

I work on Xspace, which is an internal tool for developer environments. So even though I have done the work, people still come for help, and then you have to support them. You have to see how can you improve the tool so that it solves that particular pain point for the users. You have to listen to the feedback. You have to treat it as an actual product. And even though you might not work on it every single day, you have to continue to invest in it if it is providing value to the users. 

So to answer your question around experiments and to get out of the experimental phase. You will get out of the experiment phase once you see that people are not asking for more features. People are sort of happy with the product and are not reporting any major issues. And that's when you know that you are out of that phase. Until then, you have to continue to iterate based on user feedback. But the most important thing is you have to get something out. If you don't get anything out, you know, if it's just a thought, then it does not help anyone.

**Interviewer**: Thank you. I can go on and on and on. 

**Participant D**: All right. Cool. Have a nice day. Bye.

---

*All company and participant identifiers have been anonymized in accordance with the project’s data ethics and confidentiality commitments.*

# Appendix E: Interview Transcript of Participant E

**Interview Location:** Zoom  
**Mode:** Online  
**Date:** 15/05/2025  
**Participant Role:** Lead Engineer at the case fintech

---

**Interviewer**: People often say that big data is like gold, right? There's data everywhere, in different formats, but the real value lies in bringing it all together to create a comprehensive picture. How would you say we currently use this data? Do you think we're effectively utilizing big data? Imagine a scenario where observability data merges with other data forms, leading to deeper insights. Then, observability could truly inform strategy. A CEO might realize that when the company API experiences a certain latency, something goes wrong. This insight could prompt them to increase server capacity. You can literally see how observability fits into the larger picture, enabling business decisions like, "I'm going to invest more here," or "Our infrastructure seems to be lagging here, so we'll invest there." These are financial and product decisions. So, do you believe the company is currently utilizing big data very well?

**Participant E**: Actually, we are not utilizing it well; you've just answered your own question. We need to utilize it. I think recently, within our company, we've had sessions, we've started some AI sessions, etc. I believe our direction is moving towards building a robust big data ecosystem within the company, training it with multiple teams, providing more context, and then using prompt engineering or the data for business decisions, as you mentioned. So, yes, that is next. What's next for the company is establishing big data. I'm not entirely sure what it will look like, as I'm a beginner in understanding it. But yes, we need it.

If you ask me what kind of big data we need, consider the example of a large merchant. If you gather data from various sources—and I'm giving a very rough number of sources here—you bring in data from Cloudflare, VPC flow logs, API gateway logs, other logs, metrics, and traces, consolidating everything into big data. You also integrate transaction data from the data team, from Redis, and the actual successful, committed transactions from the database, which hold the real meaning of PIV (Payment Instrument Value) and all. So, a request flow propagating from Cloudflare all the way to a processor—whatever data you emit, you collect it, consolidate it, label it, and then start querying and visualizing it. Imagine how useful it would be to see how your products are performing and how much PIV each product is generating. This is visible through the database. But again, if you want to see if you're losing PIV for a particular product, and what the reason is, collecting telemetry data and other data together, seeing the end-to-end flow, would be incredibly helpful.

I'll give an example. There were a few projects run by the platform team for our internal risk analysis tool. Let's say if merchants attempt fraudulent transactions of 1 million or 2 million, we usually decline them, right? Because that's not a valid transaction for a merchant to do within a day or two. But later on, we realized that these were real transactions that the merchant wanted to process. We were declining them through our internal fraud application. We discovered this after, I'm not sure of the exact number of days or months, but one of the product managers or principal engineers noticed a high decline rate from this merchant, manually. They asked, "Why is this happening? Are we losing PIV here? What's going on?" So, a product manager had to reach out to the merchant and ask what was happening. It turned out the merchant was legitimate and conducting proper business, so we had to accept the transactions and adjust the application rules.

This was a reactive approach. We then started increasing the PIV. Imagine if you had data from multiple sources and could see patterns of PIV improvement week over week, month over month, plus API request success rates, and the success rates of the issues you make. I don't think you would apply a hard rule on our internal fraud application because you'd know this merchant isn't even sending 5XX errors anymore; they're consistently doing 200s, and their PIV is drastically increasing from 200 to 500 to 1 million to 2 million, which means they're doing well in the market, so we shouldn't stop them. I'm just giving an example based on my context as an observability team member, but for business, it definitely helps, especially for product managers and C-level people if they want to derive certain metrics. Yes, it helps.

**Interviewer**: So, you've pretty much answered my next question. From what you're saying, you believe holistic observability makes sense. And if I look at your internal fraud application example, there's a scenario where it would have been easier to connect that loss of PIV back to our internal fraud application, but it took longer because these things aren't easily surfaced, right?

**Participant E**: Yeah, yeah, and it's a more reactive approach. Even merchants were coming back and saying, "Hey, I have sufficient funds available, I'm doing legitimate business, and the company is declining my transaction." That's when we would go and investigate what was happening.

**Interviewer**: Yes, this is a direct example of how it's one thing to know that your application is doing fine, and another to be able to tell exactly how your application is affecting the business. Our internal fraud application is working, but it's also reducing revenue for the company, and it takes time to even detect such statistics. You've answered my next question. You see the value in holistic observability, and you think AI can help, right?

**Participant E**: Yes, it definitely helps. Anyway, I think I can give you two more quick examples.

**Interviewer**: Okay.

**Participant E**: In South Africa, if you remember this year or last year, there was one merchant running child pornographic content, and they weren't able to catch him because he was showing legitimate business. It was coming from multiple URLs, etc. We have visibility through logging, like where he's coming from, which IP he's using, where he's going. We can really track it. And if you look at a recent incident in South Africa, if I remember correctly, there's a merchant from China using randomly generated emails and just doing some fraudulent transactions. We were able to find that out; one of the principal engineers or product managers discovered it. Something happened, I don't have much context, but those kinds of anomalies from merchants—you have those telemetry signals within your observability data. For example, you have certain request numbers, response numbers, and matrix numbers, but if there's a sudden increase or decrease, or a sudden increase in their email counts, that's an anomaly. We don't have that visibility with observability now. With big data, yes, we would be able to see that anomaly or predictions of merchant activity through our transactions. That's pretty much doable. Of course, we're doing it through our internal fraud application, but through observability telemetric signals, it's also doable.

**Interviewer**: Okay, so if I understand you, you're saying anomaly detection for business metrics. We can usually detect anomalies in core telemetry signals from observability data, but when it comes to business-related anomalies...

**Participant E**: Yes, correct. Correct.

**Interviewer**: It's hard.

**Participant E**: Yeah, it is hard.

**Interviewer**: Okay. But this is possible; we just don't have the current capabilities.

**Participant E**: Yes, it is possible. We just need to add more context to the telemetry signals you're emitting. And yes, you build your big data with more information and ask questions and store data for a longer period. Yes, this is possible.

**Interviewer**: I'm sorry, I feel like I've dragged this interview on for too long; I will finish soon. Okay. And then you clearly see LLMs as a tool that can help with big data management and holistic observability, right?

**Participant E**: Yeah, yeah, I can see it. These are the use cases I've given you. So those use cases, I mean, every company is trying to solve them. As I said, we have some technology dependency also. Our priority is to help engineers ensure observability is in place first. Second is the cost optimization of looking into the tool. And third is to have this big data. To emit signals to big data, we need to have open, vendor-neutral instrumentation in place so that we can send it to multiple backends in parallel. That project might take some time for the company because we still run our core services with a legacy stack. So we are in the process of upgrading it. We would do it eventually, and we have to do a lot of POCs—open standards, open telemetry, and using backends. The team is also learning; they are also evolving. So apparently, whatever projects we have within observability align with that; we are focusing on the open telemetry side of things. And we also started looking at Elastic as open source, not as a vector database, but open source. We started pushing into the logs. So we are still in the exploratory phase, I can confirm, of moving our telemetry signals to big data. And then from there, we will evolve. Yeah, use LLM to... But...

**Interviewer**: Right now, do you have a roadmap in your head, or do you have a clear plan on how we can combine—and now I'm speaking from an observability point of view—do you feel like there's a clear strategy of how we can utilize other forms of data to improve the insights that we offer? And it's okay to say no, not right now. It's okay to say, "Oh, it's in the works," but do you think that we've established that vision or even started working on it?

**Participant E**: I think we have not started working on it. There is no vision as such, but again, I have something I can share, which you might already know. I'm just waiting for management's upcoming announcements on AI usage within the company. For example, you asked, "Do we have any RAG (Retrieval-Augmented Generation) system within the company?" Let's say they come up with a big data or RAG system from the data team or engineering. That's where I would start thinking about feeding our observability data to the big data, alongside our own observability backend. For instance, our core observability backends right now are Datadog, Prometheus, and Elastic. So you send telemetry signals to these three sources, and you also feed your data to your main big data. And then from there, you take it onwards with the LLM, etc. So that's the plan. To reach there, again, I need a proxy, similar to Kafka, an open standard proxy where I can send my data pipeline to any of the sources. Right. So that's where we are working on it. Yeah. But for an AI-powered, LLM-powered observability stack, we are not ready. That's my answer. Yeah.

**Interviewer**: So you're saying there's this idea, but you need a wider infrastructure that your plans can fit into. Meaning that before we can get here, before we can improve observability with AI, the company also has to get a central AI platform, right?

**Participant E**: Yeah. Yeah. So there's one thing I could think of. I think I was demoing. I'm not sure what the possibility is there. So all we, of course, that's a long-term roadmap for the company, right? Having big data from multiple sources, you feed it, and that's just a longer "what's next" for big data within the company. But for observability, what I could think of is an MCP (Model Context Protocol) server. I believe it's going to be a real game-changer. Why? All you need is, if you are not correlating your data from multiple platforms, if you really want to see what observability is, what insights you can derive from observability through prompt engineering, I think MCP server evolution might help us. As I was showing you, I've tested it thoroughly. If you have your data on Elastic, I'll just give you an example, I'm just thinking about it. So you have the company API, logs, and legacy transactions, metric counts—both you are feeding to Elastic indices, right? And you have an Elastic MCP server, and you're using Cursor to start asking questions to two indices, logging indices and matrix indices, to ask the same questions on Spotty. How exactly is it doing? What is the anomaly? What is the transaction state? Instead of going to multiple dashboards, multiple signals, multiple things. So, readily available, people are building it. They're open-sourcing it, and they're using it in-house. Our tools are built in-house. Our Prometheus is in-house, Elastic is in-house, but we have to educate ourselves on how to use Cursor effectively, how to use this MCP server effectively, ourselves first for observability, and then start showing it to engineering teams for adoption if they are okay with it.

And again, we have to do a lot of tests on asking questions, training our MCP server itself on our questionnaire. So that is a near real-time solution I could see for observability on AI for the MCP server. I think Elastic also has a very good blog on using the MCP server. People are using it. The only problem with the MCP server is they just started it, and it's going to evolve. For example, if you look at Elastic, the Elastic MCP server will not give you aggregations, like sum, average, max, but Elastic will give you answers. Like you go and get something, it will go and get something and give you answers. However, the Prometheus one, yes, it gives you the aggregations. So, I'm closely monitoring the evolution of MCP as well. So, in parallel with big data, we also have to expedite, explore, try to implement, and try to explore ourselves first as observability and see if it is useful. All you need is data from observability, right? Using your beautiful charts or widgets or asking questions, you need it.

**Interviewer**: So, how is the MCP server different from... I'm trying to wrap my head around how the MCP server works, but what is its core strength? Is the MCP server the interface through which I query data?

**Participant E**: Yes. Just the interface, actually. So you use the Datadog queries, right? To write queries and get the data. You use PromQL to get the PromQL data. You use Elasticsearch query to get the data. So this MCP server is just a proxy. It understands what prompt query, what data query, what Elastic query it has to use to go and get the data through LLMs. Through LLMs. Obviously, it needs LLMs. Through LLMs, the MCP server goes and gets it. And you give your data source as an MCP server destination to your server, and you start asking questions. It's like it just converts your natural language to the proper query language of Datadog, Elastic, and Prometheus.

**Interviewer**: Great. I'll definitely explore it more during the course of this research. Thank you, Pointer. Okay. Now this has gone on too long, so I will just... But yes, I mean, you're the only one who could have given me this level of observability detail. So for the rest of the stuff, we can get catchy answers, right? So, I don't know if you saw my plan. I drew a conceptual diagram—now that's not what I'm saying it's going to look like, but exactly what it's going to look like—where it was like a conceptual diagram of what I think a system that facilitates holistic observability can look like. I will share my screen just to refresh your knowledge, but I would like to get your idea of what you think about this system.

Yeah. So I don't know how much you know about these technologies, but this is supposed to use the RAG architecture and LangChain, right? And LangChain is really just chaining LLM processes. It just makes... So when a user gives a query... how conversant are you with things like agentic AI and how you can use tools within AI? So the central concept is I have a telemetry agent, I have a business metric agent, I have a knowledge base agent. So my idea of holistic observability is that we can get telemetry metrics from maybe a platform like Datadog, we can get business metrics from maybe a database, and we can just store some of all this operational analogy, like you say, "Oh, some people know, key engineers know." They can just be in a vector database, right? And these agents can use tools. Like, so just like you said, bringing all of this data into one house. This agent knows how to use the user query and determine, based on what this user has said, which of these data sources should I ask? So it can query these data sources, bring them together, and respond via the LLM. So, can we quickly see how a telemetry agent can also talk to another, maybe Prometheus, or maybe this can even be like your MCP servers, based on what you think it is, right? So the MCP server can talk to the database. So like, I want to let you go, but what do you think of the overall architecture? Do you think this is something that benefits the community?

**Participant E**: Yeah, I think this will be useful. Yeah. Yeah, I mean, I haven't studied much about LangChain and RAG, etc. I try based on my requirements. I said, like MCP looks interesting. I started trying it. So usually other AI-based tools, I just try for prompt engineering only, but this looks good only because you have consolidated everything we need, mainly the knowledge base. You feed more context to the metadata database and then the meta base and the Datadog side of it, the root side of it. So data processing and correlation, is it a big database?

**Interviewer**: Yeah, it's really LangChain, and that's like the beauty of LangChain. So LangChain is doing the heavy lifting, right? LangChain knows that, "Oh, because you sent me this query, I want to talk to this guy. And when I get it from this guy, I'm going to string this, this, and this and pass it to the LLM for context." Right. So, you may have to code some of the instructions, but LangChain automatically brings all of this data, puts it together, and passes it to the LLM.

**Participant E**: Okay. Okay. So it's not a separate database.

**Interviewer**: Yes, you don't know the core technologies, but you think that the structure makes sense.

**Participant E**: Yeah, I think this is okay. This looks good. The only thing I think is missing is from the infrastructure layer here, mainly from API Gateway, Cloudflare, and the core company API. But again, you have the metrics in observability, so that should be fine.

**Interviewer**: Yeah, I mean, it's also a very extendable system. Like, you can just add another agent and get... So this one was just a simplified version. But would you have any concerns about that system? You think it's good, and you highlighted the benefit, but do you have any concerns? Do you see cases where it can be a problem or where there are any specific risks?

**Participant E**: I don't see any problems, but I'm not sure how the implementation goes because I haven't experienced those technologies, but combining everything through LangChain. So if you have use cases, people have implemented it. If it is working, yeah, they can try it out. Why can't that solve a lot of problems? Like if you see the CEO question on one of the AI calls, initially there was a complaint from one of the merchants for refunds; we were not able to do it on time, etc. We have to go to multiple things through emails, through Watchtower, and it needs context from an engineer. If you feed that context to your database, and you have the data on the database, and if the user is calling, start asking questions, and it can go to VectorDB and the database, and it can give the answers. Yeah, it would definitely be useful learning.

**Interviewer**: Great, I will let you go now because we are not restarting this meeting, and if I need to schedule another call, I will, but I think I've kept you for too long. Yeah, question, and hopefully through it, someone like Drew... but yeah, so I've heard things like, "Okay, I think this is going to be nice, but I don't know a lot about these technologies." How do you think that can change that? Because, like, you know, there's so much that can be done with LLMs, but the truth is that employees are just trying to catch on. But do you have any ideas on how maybe the company can just help employees? So, what would be useful for you now, or yeah, I've shown you an interesting system, what would be useful for you to be able to try and make it work?

**Participant E**: It's a very good question; I was even thinking about it here. So if you see, let's say, if I want to learn a new technology for myself, right? An example is like I started working on Elastic. So Elastic was a little easy for me since I was working on Splunk, same for logging and all. But if it's a new thing, I would take a very structured approach to understanding why I need it, and how to implement, like five "why" questions: why, why, why, why, why. Okay, why do I need it? I need it for logging. And why should I implement it? And if the implementation is correct or not correct, and how can I validate it? So those kinds of questions I myself ask and get into Udemy, YouTube, mainly Elastic documentation, etc. And I start rolling the ball.

But when it comes to AI, the problem is there is no structured way of learning. So if you ask me what I need, I don't want to understand how an LLM can be built; I don't want to understand. Like if you ask me, "Hey, do you drive your car?" Yes, I drive my car comfortably. I have been driving in Dubai for 10 years, and I know when the tires need attention, I can go to a petrol station, and they can fix it. When the gas is low, I can fill it, and when there is a maintenance thing. So all I need is for the car to take care of me going from destination A to B and carrying my family, etc. The same way for LLM. I don't want to build a new LLM. I don't want to understand how an LLM can be built, etc. All I want is to solve my problems. Where is my domain? 

As I said, my problems, observability problems, what's next for observability in terms of AI. And to solve that, what do I need to learn, what do I need to understand? For that, I don't have a learning curve. Even I have to go to the web and maybe join a couple of Drew sessions. I think I have been joining. Still 100% not clear. I'm just jumping in. I'm just trying myself. But a proper course curriculum—not a course curriculum, I would say. Okay, being an experienced engineer, what level of AI, LLMs you should know, and what level of implementations you should know in your hardcore product development lifecycle. 

Being an infrastructure engineer, what you should know. If you have a proper course curriculum followed by exercises and doing some POCs, that would really help every engineer within the company. That I could think that they are missing it. They're just going and asking prompts to ChatGPT, Gemini, and I started using Cursor. It's really a cool one. It's doing a lot of stuff for me. But yeah, that is something we are missing. Like having a strong foundation for engineers at your level, at your department. Let's say if you see Steve, Steve from the UserOps team. He has his different requirements, and being a member for observability, I have different requirements. And being a Richard from engineering, he has different requirements for solving problems with AI. So yeah, that we are missing.

**Interviewer**: So it sounds to me like you're saying, okay, a clearer learning curve will help.

**Participant E**: Yeah. Then some kind...

**Interviewer**: Of, I mean, if I didn't get it well, then you're saying some kind of abstraction around LLMs so that it can maybe even fit into different people's use cases, like what you said about your car, right? You don't need to know every detail, but you can just plug and play into certain things, and it moves, right? So some form of abstractions that just allow you, "Okay, this LLM system is here, I just need to do this little tweak so that it works for my own use case," right?

**Participant E**: Correct, correct.

**Interviewer**: I am inclined to tell you bye-bye here. I can't go on and on and on, but I will stop here. I really appreciate your time. 

---

*All company and participant identifiers have been anonymized in accordance with the project’s data ethics and confidentiality commitments.*
